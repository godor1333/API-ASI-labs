import numpy as np
import random
import pickle
from pathlib import Path

class EpsilonGreedyAgent:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π Œµ-greedy –∞–≥–µ–Ω—Ç –¥–ª—è Space Invaders"""
    
    def __init__(self, num_actions, epsilon=0.3, alpha=0.2, gamma=0.95):
        self.num_actions = num_actions
        self.epsilon = epsilon  # –ù–∞—á–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ exploration
        self.alpha = alpha      # –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
        self.gamma = gamma      # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –¥–∏—Å–∫–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        self.q_table = {}       # –¢–∞–±–ª–∏—Ü–∞ Q-–∑–Ω–∞—á–µ–Ω–∏–π
        self.state_counts = {}  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–µ—â–µ–Ω–∏–π —Å–æ—Å—Ç–æ—è–Ω–∏–π
        
        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.learning_history = []
        
        print(f"ü§ñ –ê–≥–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: Œµ={epsilon}, Œ±={alpha}, Œ≥={gamma}")
    
    def get_action(self, state, step=0):
        """
        –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è –ø–æ Œµ-greedy —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        —Å —É—á–µ—Ç–æ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        state_key = self._state_to_key(state)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Q-–∑–Ω–∞—á–µ–Ω–∏–π –∏ —Å—á–µ—Ç—á–∏–∫–æ–≤ –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if state_key not in self.q_table:
            self.q_table[state_key] = np.zeros(self.num_actions)
            self.state_counts[state_key] = 0
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ—Å–µ—â–µ–Ω–∏–π
        self.state_counts[state_key] = self.state_counts.get(state_key, 0) + 1
        
        # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π epsilon (—É–º–µ–Ω—å—à–∞–µ—Ç—Å—è —Å –æ–ø—ã—Ç–æ–º)
        state_visits = self.state_counts[state_key]
        dynamic_epsilon = self.epsilon / (1 + 0.01 * state_visits)
        
        # Œµ-greedy –≤—ã–±–æ—Ä
        if random.random() < dynamic_epsilon:
            # –°–ª—É—á–∞–π–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
            action = random.randint(0, self.num_actions - 1)
            exploration_type = "random"
        else:
            # –ñ–∞–¥–Ω–∞—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è
            q_values = self.q_table[state_key]
            max_q = np.max(q_values)
            
            # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–µ–π—Å—Ç–≤–∏–π —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Q, –≤—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω–æ–µ –∏–∑ –Ω–∏—Ö
            best_actions = np.where(q_values == max_q)[0]
            action = np.random.choice(best_actions) if len(best_actions) > 0 else random.randint(0, self.num_actions - 1)
            exploration_type = "greedy"
        
        # –ó–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏—é (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
        if step % 10 == 0:  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ –∫–∞–∂–¥—ã–π —à–∞–≥
            self.learning_history.append({
                'step': step,
                'state_key': state_key[:20] if len(state_key) > 20 else state_key,
                'action': action,
                'exploration': exploration_type,
                'epsilon_used': dynamic_epsilon,
                'state_visits': state_visits
            })
        
        return action
    
    def update_q_values(self, state, action, reward, next_state):
        """–£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ Q-—Ç–∞–±–ª–∏—Ü—ã"""
        state_key = self._state_to_key(state)
        next_state_key = self._state_to_key(next_state)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if state_key not in self.q_table:
            self.q_table[state_key] = np.zeros(self.num_actions)
        if next_state_key not in self.q_table:
            self.q_table[next_state_key] = np.zeros(self.num_actions)
        
        # –¢–µ–∫—É—â–µ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ
        current_q = self.q_table[state_key][action]
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ Q-–∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        next_max_q = np.max(self.q_table[next_state_key])
        
        # –§–æ—Ä–º—É–ª–∞ Q-learning —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º alpha
        visits = self.state_counts.get(state_key, 1)
        adaptive_alpha = self.alpha / (1 + 0.001 * visits)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ Q-–∑–Ω–∞—á–µ–Ω–∏—è
        new_q = current_q + adaptive_alpha * (reward + self.gamma * next_max_q - current_q)
        self.q_table[state_key][action] = new_q
        
        return new_q, adaptive_alpha
    
    def _state_to_key(self, state):
        """–£–ø—Ä–æ—â—ë–Ω–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –≤ –∫–ª—é—á"""
        # –î–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–∞–±–æ—Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ö–µ—à
        try:
            if hasattr(state, 'tobytes'):
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ bytes –∏ –±–µ—Ä—ë–º —Ö–µ—à
                return str(hash(state.tobytes()))
            
            # –ï—Å–ª–∏ —ç—Ç–æ numpy array
            if hasattr(state, 'shape'):
                # –ë–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è MinAtar
                if len(state.shape) == 3:
                    # –ë–∏–Ω–∞—Ä–∏–∑—É–µ–º –∏ —Å–æ–∑–¥–∞—ë–º —Å—Ç—Ä–æ–∫—É
                    binary_state = (state > 0).astype(int)
                    return str(binary_state.tobytes()[:100])  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 100 –±–∞–π—Ç
        except:
            pass
        
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
        return str(state)
    
    def save_model(self, filepath="models/epsilon_greedy_model.pkl"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        try:
            Path(filepath).parent.mkdir(parents=True, exist_ok=True)
            
            model_data = {
                'q_table': self.q_table,
                'state_counts': self.state_counts,
                'params': {
                    'epsilon': self.epsilon,
                    'alpha': self.alpha,
                    'gamma': self.gamma,
                    'num_actions': self.num_actions
                },
                'learning_history': self.learning_history[-100:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filepath}")
            print(f"  –†–∞–∑–º–µ—Ä Q-—Ç–∞–±–ª–∏—Ü—ã: {len(self.q_table)} —Å–æ—Å—Ç–æ—è–Ω–∏–π")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self, filepath="models/epsilon_greedy_model.pkl"):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.q_table = model_data['q_table']
            self.state_counts = model_data.get('state_counts', {})
            
            if 'learning_history' in model_data:
                self.learning_history = model_data['learning_history']
            
            print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filepath}")
            print(f"  –†–∞–∑–º–µ—Ä Q-—Ç–∞–±–ª–∏—Ü—ã: {len(self.q_table)} —Å–æ—Å—Ç–æ—è–Ω–∏–π")
            
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def get_stats(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∞–≥–µ–Ω—Ç–∞"""
        total_states = len(self.q_table)
        total_visits = sum(self.state_counts.values()) if self.state_counts else 0
        
        # –ê–Ω–∞–ª–∏–∑ exploration/exploitation
        if self.learning_history:
            recent_history = self.learning_history[-50:] if len(self.learning_history) > 50 else self.learning_history
            explorations = [h.get('exploration', 'unknown') for h in recent_history]
            random_actions = sum(1 for e in explorations if e == 'random')
            exploration_rate = random_actions / len(explorations) * 100 if explorations else 0
        else:
            exploration_rate = 0
        
        return {
            'total_states': total_states,
            'total_visits': total_visits,
            'avg_visits_per_state': total_visits / total_states if total_states > 0 else 0,
            'exploration_rate': exploration_rate,
            'current_epsilon': self.epsilon,
            'q_table_size': sum(len(q) for q in self.q_table.values()) if self.q_table else 0
        }