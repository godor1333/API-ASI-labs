import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from env_wrapper.minatar_wrapper import MinAtarWrapper
from agents.epsilon_greedy import EpsilonGreedyAgent
from agents.random_agent import RandomAgent
from object_detection.simple_detector import SimpleObjectDetector
from analytics.metrics import GameMetrics
from analytics.visualizer import GameVisualizer
from utils.logger import GameLogger
import numpy as np
import cv2
from pathlib import Path
import time


def main():
    """–û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞"""
    print("=" * 70)
    print("üöÄ –ü—Ä–æ–µ–∫—Ç: –ê–Ω–∞–ª–∏–∑ –ø–æ–≤–µ–¥–µ–Ω–∏—è AI-–±–æ—Ç–æ–≤ –≤ 2D-—à—É—Ç–µ—Ä–µ")
    print("=" * 70)

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
    logger = GameLogger()
    logger.log_training_start({"game": "space_invaders", "agent": "epsilon_greedy"})

    try:
        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–µ–¥—ã
        print("\nüéÆ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–≥—Ä—ã...")
        env_wrapper = MinAtarWrapper("space_invaders")
        logger.logger.info(f"–°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞: {env_wrapper.game_name}")

        # 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
        print("ü§ñ –°–æ–∑–¥–∞–Ω–∏–µ Œµ-greedy –∞–≥–µ–Ω—Ç–∞...")
        agent = EpsilonGreedyAgent(
            num_actions=env_wrapper.get_num_actions(),
            epsilon=0.2
        )
        logger.logger.info(f"–ê–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: Œµ={agent.epsilon}")

        # 4. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ–±—ä–µ–∫—Ç–æ–≤
        print("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ–±—ä–µ–∫—Ç–æ–≤...")
        detector = SimpleObjectDetector()

        # 5. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        print("üìä –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏—Ç–∏–∫–∏...")
        metrics = GameMetrics()
        visualizer = GameVisualizer()

        # 6. –û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
        print("\nüéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        num_episodes = 5  # –ù–∞—á–Ω–µ–º —Å 5 —ç–ø–∏–∑–æ–¥–æ–≤
        steps_per_episode = 100

        # –î–ª—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã
        all_positions = []

        for episode in range(num_episodes):
            logger.log_episode_start(episode, agent.epsilon)
            print(f"\nüìà –≠–ø–∏–∑–æ–¥ {episode + 1}/{num_episodes}")

            state = env_wrapper.reset()
            total_reward = 0
            episode_positions = []

            for step in range(steps_per_episode):
                # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
                detected_objects = detector.detect(state)

                # –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è –∞–≥–µ–Ω—Ç–æ–º
                action = agent.get_action(state, step)

                # –®–∞–≥ –≤ —Å—Ä–µ–¥–µ
                next_state, reward, terminated, _ = env_wrapper.step(action)
                total_reward += reward

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
                agent.update_q_values(state, action, reward, next_state)

                # –ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫
                metrics.record_step(
                    episode=episode,
                    step=step,
                    action=action,
                    reward=reward,
                    objects_detected=len(detected_objects),
                    state=state
                )

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                logger.log_step(episode, step, action, reward, total_reward, len(detected_objects))

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏ –∏–≥—Ä–æ–∫–∞ –¥–ª—è —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã
                for obj in detected_objects:
                    if obj['type'] == 'player':
                        episode_positions.append(obj['position'])
                        all_positions.append(obj['position'])

                state = next_state

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–¥—Ä–∞ –∫–∞–∂–¥—ã–π 20-–π —à–∞–≥ –≤ –ø–µ—Ä–≤–æ–º —ç–ø–∏–∑–æ–¥–µ
                if episode == 0 and step % 20 == 0:
                    img = detector.visualize_detection(state, detected_objects)
                    cv2.imwrite(f"training_frame_ep{episode}_step{step}.png", img)

                if terminated:
                    logger.logger.info(f"–ò–≥—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –Ω–∞ —à–∞–≥–µ {step}")
                    break

            logger.log_episode_end(episode, total_reward, min(step, steps_per_episode), agent.epsilon)
            print(f"  üèÜ –ù–∞–≥—Ä–∞–¥–∞ —ç–ø–∏–∑–æ–¥–∞: {total_reward:.1f}")
            print(f"  üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {metrics.get_avg_objects(episode):.1f}/—à–∞–≥")

            # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ epsilon
            agent.epsilon = max(0.05, agent.epsilon * 0.95)

        # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        results_dir = Path("training_results")
        results_dir.mkdir(exist_ok=True)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
        metrics.save_report(results_dir / "training_report.txt")
        metrics.plot_training_progress(results_dir / "training_plots.png")

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤—ã—Ö –∫–∞—Ä—Ç
        if all_positions:
            heatmap = visualizer.create_heatmap(
                all_positions,
                grid_size=(10, 10),
                title="–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –ø–æ–∑–∏—Ü–∏–π –∏–≥—Ä–æ–∫–∞"
            )
            visualizer.save_visualization(
                heatmap,
                results_dir / "player_heatmap.png"
            )

        logger.log_training_end({
            "episodes": num_episodes,
            "final_epsilon": agent.epsilon,
            "total_positions": len(all_positions)
        })

        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –ø–∞–ø–∫–µ: {results_dir}/")

        # 8. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        print("\nüéÆ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞...")
        test_agent_performance(env_wrapper, agent, detector, logger)

        # 9. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º
        print("\nüìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–ª—É—á–∞–π–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º...")
        compare_with_random(env_wrapper, detector, metrics)

    except Exception as e:
        logger.logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ø–æ—Ç–æ–∫–µ: {e}")
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()


def test_agent_performance(env, agent, detector, logger):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞"""
    print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞...")

    state = env.reset()
    total_reward = 0
    test_positions = []

    # –û—Ç–∫–ª—é—á–∞–µ–º exploration –¥–ª—è —Ç–µ—Å—Ç–∞
    original_epsilon = agent.epsilon
    agent.epsilon = 0.0

    for step in range(50):
        # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
        objects = detector.detect(state)

        # –î–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞
        action = agent.get_action(state, step)

        # –®–∞–≥
        next_state, reward, terminated, _ = env.step(action)
        total_reward += reward

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏
        for obj in objects:
            if obj['type'] == 'player':
                test_positions.append(obj['position'])

        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        if step % 10 == 0:
            frame = detector.visualize_detection(state, objects)
            cv2.imwrite(f"test_frame_{step:03d}.png", frame)
            print(f"  –®–∞–≥ {step}: –î–µ–π—Å—Ç–≤–∏–µ={action}, –ù–∞–≥—Ä–∞–¥–∞={reward:.1f}")

        state = next_state

        if terminated:
            print(f"  üéÆ –ò–≥—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –Ω–∞ —à–∞–≥–µ {step}")
            break

    # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º epsilon
    agent.epsilon = original_epsilon

    print(f"üèÅ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω! –ò—Ç–æ–≥–æ–≤–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.1f}")
    logger.logger.info(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞: –Ω–∞–≥—Ä–∞–¥–∞={total_reward:.1f}, —à–∞–≥–æ–≤={step}")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã —Ç–µ—Å—Ç–∞
    if test_positions:
        visualizer = GameVisualizer()
        heatmap = visualizer.create_heatmap(
            test_positions,
            grid_size=(10, 10),
            title="–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞"
        )
        visualizer.save_visualization(
            heatmap,
            "test_heatmap.png"
        )


def compare_with_random(env, detector, metrics):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å —Å–ª—É—á–∞–π–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º"""
    print("üé≤ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞...")

    random_agent = RandomAgent(env.get_num_actions())
    state = env.reset()
    total_reward = 0

    for step in range(50):
        # –î–µ–π—Å—Ç–≤–∏–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        action = random_agent.get_action()

        # –®–∞–≥
        next_state, reward, terminated, _ = env.step(action)
        total_reward += reward

        state = next_state

        if terminated:
            break

    print(f"üé≤ –°–ª—É—á–∞–π–Ω—ã–π –∞–≥–µ–Ω—Ç: –Ω–∞–≥—Ä–∞–¥–∞ = {total_reward:.1f}")

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –æ–±—É—á–µ–Ω–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º
    trained_stats = metrics.get_training_summary()
    if trained_stats and trained_stats['episodes']:
        last_episode_reward = trained_stats['episodes'][-1]['total_reward']
        improvement = ((total_reward - last_episode_reward) / abs(last_episode_reward) * 100
                       if last_episode_reward != 0 else 0)

        print(f"\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        print(f"  üéØ –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç (–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ø–∏–∑–æ–¥): {last_episode_reward:.1f}")
        print(f"  üé≤ –°–ª—É—á–∞–π–Ω—ã–π –∞–≥–µ–Ω—Ç: {total_reward:.1f}")
        print(f"  üìà –†–∞–∑–Ω–∏—Ü–∞: {total_reward - last_episode_reward:+.1f}")
        print(f"  üìä –ò–∑–º–µ–Ω–µ–Ω–∏–µ: {improvement:+.1f}%")


if __name__ == "__main__":
    main()