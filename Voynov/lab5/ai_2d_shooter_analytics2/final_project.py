import sys
import os

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from env_wrapper.minatar_wrapper import MinAtarWrapper
from agents.epsilon_greedy import EpsilonGreedyAgent
from agents.random_agent import RandomAgent
from analytics.metrics import GameMetrics
from analytics.visualizer import GameVisualizer
import numpy as np
from pathlib import Path
import cv2
import time
import json
from datetime import datetime

print("=" * 80)
print("üöÄ –§–ò–ù–ê–õ–¨–ù–´–ô –ü–†–û–ï–ö–¢: –ê–Ω–∞–ª–∏–∑ –ø–æ–≤–µ–¥–µ–Ω–∏—è AI-–±–æ—Ç–æ–≤ –≤ 2D-—à—É—Ç–µ—Ä–µ")
print("=" * 80)

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –Ω–æ–≤—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≠—Ç–∞–ø–∞ 2
try:
    from agents.dqn_agent import DQNAgent

    DQN_AVAILABLE = True
except ImportError as e:
    DQN_AVAILABLE = False
    print(f"‚ö†Ô∏è DQN –∞–≥–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")

try:
    from utils.model_interface import ModelInterface
    from agents.agent_wrapper import AgentWrapper
    from object_detection.detector_wrapper import DetectorWrapper
    from analytics.inference_visualizer import InferenceVisualizer

    TESTING_FRAMEWORK_AVAILABLE = True
except ImportError as e:
    TESTING_FRAMEWORK_AVAILABLE = False
    print(f"‚ö†Ô∏è –§—Ä–µ–π–º–≤–æ—Ä–∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")


# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏ numpy
class NumpyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.bool_):
            return bool(obj)
        else:
            return super().default(obj)


def create_project_structure():
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞"""
    folders = ["results", "models", "logs", "screenshots", "videos", "reports"]
    for folder in folders:
        Path(folder).mkdir(exist_ok=True)
    print("üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞ —Å–æ–∑–¥–∞–Ω–∞")


class VideoRecorder:
    """–ü—Ä–æ—Å—Ç–æ–π —Ä–µ–∫–æ—Ä–¥–µ—Ä –¥–ª—è –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ"""

    def __init__(self, filename="videos/gameplay.mp4", fps=10.0, frame_size=(160, 160)):
        self.filename = filename
        self.fps = fps
        self.frame_size = frame_size
        self.writer = None
        Path("videos").mkdir(exist_ok=True)

    def start(self):
        """–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"""
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        self.writer = cv2.VideoWriter(self.filename, fourcc, self.fps, self.frame_size)
        print(f"üé• –ù–∞—á–∞—Ç–∞ –∑–∞–ø–∏—Å—å –≤–∏–¥–µ–æ: {self.filename}")

    def record_frame(self, frame):
        """–ó–∞–ø–∏—Å–∞—Ç—å –∫–∞–¥—Ä"""
        if self.writer is not None:
            # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω—É–∂–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É
            if frame.shape[:2] != self.frame_size:
                frame = cv2.resize(frame, self.frame_size)
            self.writer.write(frame)

    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å"""
        if self.writer is not None:
            self.writer.release()
            print(f"üíæ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {self.filename}")


class ThreatAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —É–≥—Ä–æ–∑"""

    def __init__(self, grid_size=(10, 10)):
        self.grid_size = grid_size
        self.threat_map = np.zeros(grid_size, dtype=np.float32)
        self.detected_threats = []
        self.missed_threats = []

    def analyze_frame(self, detected_objects, agent_action, reward):
        """–ê–Ω–∞–ª–∏–∑ —É–≥—Ä–æ–∑ –≤ –∫–∞–¥—Ä–µ"""
        threats = []

        for obj in detected_objects:
            if obj['type'] in ['enemy', 'enemy_bullet', 'unknown']:
                threats.append(obj)
                x, y = obj['position']
                if 0 <= x < self.grid_size[1] and 0 <= y < self.grid_size[0]:
                    self.threat_map[y, x] += 1

        # –ï—Å–ª–∏ –±—ã–ª–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞, –Ω–æ —É–≥—Ä–æ–∑—ã –±—ã–ª–∏ - —Å—á–∏—Ç–∞–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–π —É–≥—Ä–æ–∑–æ–π
        if reward < 0 and threats:
            self.missed_threats.append({
                'step': len(self.detected_threats),
                'threats': len(threats),
                'action': int(agent_action),
                'reward': float(reward)
            })

        self.detected_threats.extend(threats)
        return len(threats)

    def generate_threat_report(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ–± —É–≥—Ä–æ–∑–∞—Ö"""
        total_threats = len(self.detected_threats)
        missed_threats = len(self.missed_threats)

        if total_threats > 0:
            miss_rate = missed_threats / total_threats * 100
        else:
            miss_rate = 0.0

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º threat_map –¥–ª—è JSON
        threat_map_list = self.threat_map.astype(float).tolist()

        return {
            'total_threats': int(total_threats),
            'missed_threats': int(missed_threats),
            'miss_rate_percent': float(miss_rate),
            'threat_map': threat_map_list,
            'high_threat_zones': self._find_high_threat_zones()
        }

    def _find_high_threat_zones(self):
        """–ù–∞–π—Ç–∏ –∑–æ–Ω—ã —Å –≤—ã—Å–æ–∫–æ–π –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏–µ–π —É–≥—Ä–æ–∑"""
        if self.threat_map.max() == 0:
            return []

        threshold = self.threat_map.mean() + self.threat_map.std()
        zones = np.where(self.threat_map > threshold)

        high_zones = []
        for y, x in zip(zones[0], zones[1]):
            high_zones.append({
                'x': int(x),
                'y': int(y),
                'threat_level': float(self.threat_map[y, x])
            })

        return high_zones


class EfficiencyCalculator:
    """–ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –∏–Ω–¥–µ–∫—Å–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""

    def __init__(self):
        self.stats = {
            'total_shots': 0,
            'hits': 0,
            'near_misses': 0,
            'reaction_times': [],
            'survival_time': 0.0,
            'damage_taken': 0.0,
            'damage_dealt': 0.0
        }
        self.last_action_time = time.time()

    def update(self, action, reward, objects_detected, step_duration):
        """–û–±–Ω–æ–≤–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        # –†–µ–∞–∫—Ü–∏—è (–≤—Ä–µ–º—è –º–µ–∂–¥—É —É–≥—Ä–æ–∑–æ–π –∏ –¥–µ–π—Å—Ç–≤–∏–µ–º)
        reaction_time = time.time() - self.last_action_time
        self.stats['reaction_times'].append(float(reaction_time))
        self.last_action_time = time.time()

        # –ê–Ω–∞–ª–∏–∑ –¥–µ–π—Å—Ç–≤–∏–π (Space Invaders: –¥–µ–π—Å—Ç–≤–∏–µ 1 - –≤—ã—Å—Ç—Ä–µ–ª)
        if action == 1:  # –í—ã—Å—Ç—Ä–µ–ª
            self.stats['total_shots'] += 1
            if reward > 0:
                self.stats['hits'] += 1
                self.stats['damage_dealt'] += float(reward)
            elif reward == 0:
                self.stats['near_misses'] += 1

        # –£—Ä–æ–Ω
        if reward < 0:
            self.stats['damage_taken'] += abs(float(reward))

        # –í—Ä–µ–º—è –≤—ã–∂–∏–≤–∞–Ω–∏—è
        self.stats['survival_time'] += float(step_duration)

    def calculate_efficiency_index(self):
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∏–Ω–¥–µ–∫—Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        if self.stats['total_shots'] == 0:
            accuracy = 0.0
        else:
            accuracy = self.stats['hits'] / self.stats['total_shots'] * 100

        if len(self.stats['reaction_times']) > 0:
            avg_reaction = np.mean(self.stats['reaction_times'])
            reaction_score = max(0.0, 100 - (avg_reaction * 1000))  # –ß–µ–º –±—ã—Å—Ç—Ä–µ–µ, —Ç–µ–º –ª—É—á—à–µ
        else:
            reaction_score = 50.0

        # –í—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å (—á–µ–º –¥–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)
        survival_score = min(100.0, self.stats['survival_time'] * 10)

        # –ò–∑–±–µ–≥–∞–Ω–∏–µ —É—Ä–æ–Ω–∞
        if self.stats['damage_taken'] > 0:
            avoidance_score = max(0.0, 100 - self.stats['damage_taken'] * 10)
        else:
            avoidance_score = 100.0

        # –û–±—â–∏–π –∏–Ω–¥–µ–∫—Å
        efficiency_index = (
                accuracy * 0.3 +  # –¢–æ—á–Ω–æ—Å—Ç—å 30%
                reaction_score * 0.25 +  # –†–µ–∞–∫—Ü–∏—è 25%
                survival_score * 0.25 +  # –í—ã–∂–∏–≤–∞–µ–º–æ—Å—Ç—å 25%
                avoidance_score * 0.2  # –ò–∑–±–µ–≥–∞–Ω–∏–µ 20%
        )

        return {
            'efficiency_index': float(efficiency_index),
            'accuracy_percent': float(accuracy),
            'reaction_score': float(reaction_score),
            'survival_score': float(survival_score),
            'avoidance_score': float(avoidance_score),
            'detailed_stats': {
                'total_shots': int(self.stats['total_shots']),
                'hits': int(self.stats['hits']),
                'near_misses': int(self.stats['near_misses']),
                'avg_reaction_time': float(
                    np.mean(self.stats['reaction_times']) if self.stats['reaction_times'] else 0),
                'survival_time': float(self.stats['survival_time']),
                'damage_taken': float(self.stats['damage_taken']),
                'damage_dealt': float(self.stats['damage_dealt'])
            }
        }


def initialize_detector():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ–±—ä–µ–∫—Ç–æ–≤"""
    print("\nüîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –æ–±—ä–µ–∫—Ç–æ–≤...")

    USE_ADVANCED_DETECTOR = True
    DETR_THRESHOLD = 0.2

    if USE_ADVANCED_DETECTOR:
        try:
            from object_detection.detector_factory import DetectorFactory
            detector = DetectorFactory.create_detector(detector_type="auto", use_gpu=False)
            if hasattr(detector, 'model') and detector.model is not None:
                detector_type = "DETR"
                print(f"   ‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä: DETR (–ø–æ—Ä–æ–≥: {DETR_THRESHOLD})")
            else:
                detector_type = "simple"
                from object_detection.simple_detector import SimpleObjectDetector
                detector = SimpleObjectDetector()
                print(f"   ‚ö†Ô∏è –î–µ—Ç–µ–∫—Ç–æ—Ä: –ø—Ä–æ—Å—Ç–æ–π (DETR –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è)")
        except Exception as e:
            print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ DETR: {e}")
            from object_detection.simple_detector import SimpleObjectDetector
            detector = SimpleObjectDetector()
            detector_type = "simple"
            print(f"   ‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä: –ø—Ä–æ—Å—Ç–æ–π (fallback)")
    else:
        from object_detection.simple_detector import SimpleObjectDetector
        detector = SimpleObjectDetector()
        detector_type = "simple"
        print(f"   ‚úÖ –î–µ—Ç–µ–∫—Ç–æ—Ä: –ø—Ä–æ—Å—Ç–æ–π")

    return detector, detector_type


def run_final_project():
    """–§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è –ø—Ä–æ–µ–∫—Ç–∞ —Å DETR, DQN –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π"""

    # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    create_project_structure()

    # 1. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –î–ï–¢–ï–ö–¢–û–†–ê
    detector, detector_type = initialize_detector()

    # 2. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ò–ì–†–´
    print("\nüéÆ 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–≥—Ä—ã Space Invaders...")
    env = MinAtarWrapper("space_invaders")
    print(f"   ‚úÖ –ò–≥—Ä–∞: Space Invaders")
    print(f"   ‚úÖ –î–µ–π—Å—Ç–≤–∏–π: {env.get_num_actions()}")
    print(f"   ‚úÖ –†–∞–∑–º–µ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è: {env.get_state_shape()}")

    # 3. –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ê–ù–ê–õ–ò–¢–ò–ö–ò
    print("\nüìä 3. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º –∞–Ω–∞–ª–∏–∑–∞...")

    threat_analyzer = ThreatAnalyzer()
    efficiency_calc = EfficiencyCalculator()
    metrics = GameMetrics()
    visualizer = GameVisualizer()

    # –í–∏–¥–µ–æ—Ä–µ–∫–æ—Ä–¥–µ—Ä —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏–º–µ–Ω–µ–º
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    video_recorder = VideoRecorder(
        filename=f"videos/gameplay_{timestamp}.mp4",
        fps=10.0,
        frame_size=(320, 320)
    )

    # –¢–µ—Å—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏
    test_state = env.reset()
    test_objects = detector.detect(test_state)

    # –î–ª—è DETR: –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–µ–ª –æ–±—ä–µ–∫—Ç—ã, –ø—Ä–æ–±—É–µ–º —Å –¥—Ä—É–≥–∏–º –ø–æ—Ä–æ–≥–æ–º
    if detector_type == "DETR" and len(test_objects) == 0:
        print("   ‚ö†Ô∏è DETR –Ω–µ –Ω–∞—à–µ–ª –æ–±—ä–µ–∫—Ç—ã, –ø—Ä–æ–±—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –¥–µ—Ç–µ–∫—Ç–æ—Ä...")
        from object_detection.simple_detector import SimpleObjectDetector
        detector = SimpleObjectDetector()
        detector_type = "simple_fallback"
        test_objects = detector.detect(test_state)

    print(f"   ‚úÖ –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –æ–±—ä–µ–∫—Ç–æ–≤: {len(test_objects)}")
    if test_objects:
        for obj in test_objects[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –æ–±—ä–µ–∫—Ç–∞
            print(f"      - {obj['type']} –≤ –ø–æ–∑–∏—Ü–∏–∏ {obj['position']}")

    # 4. –°–û–ó–î–ê–ù–ò–ï –ê–ì–ï–ù–¢–û–í
    print("\nü§ñ 4. –°–æ–∑–¥–∞–Ω–∏–µ RL-–∞–≥–µ–Ω—Ç–æ–≤...")
    num_actions = env.get_num_actions()

    random_agent = RandomAgent(num_actions)
    epsilon_agent = EpsilonGreedyAgent(
        num_actions=num_actions,
        epsilon=0.3,
        alpha=0.1,
        gamma=0.9
    )

    print(f"   ‚úÖ –°–ª—É—á–∞–π–Ω—ã–π –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
    print(f"   ‚úÖ Œµ-greedy –∞–≥–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: Œµ={epsilon_agent.epsilon}")

    # 5. –ë–´–°–¢–†–û–ï –û–ë–£–ß–ï–ù–ò–ï (3 —ç–ø–∏–∑–æ–¥–∞)
    print("\nüìö 5. –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ Œµ-greedy –∞–≥–µ–Ω—Ç–∞ (3 —ç–ø–∏–∑–æ–¥–∞)...")

    training_start = time.time()

    for episode in range(3):
        state = env.reset()
        total_reward = 0.0
        episode_objects = []
        step_start_time = time.time()

        for step in range(30):
            # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤
            detected_objects = detector.detect(state)

            # –ê–Ω–∞–ª–∏–∑ —É–≥—Ä–æ–∑
            threats_detected = threat_analyzer.analyze_frame(
                detected_objects,
                None,
                0
            )

            # –í—ã–±–æ—Ä –¥–µ–π—Å—Ç–≤–∏—è
            action = epsilon_agent.get_action(state, step)

            # –®–∞–≥ –≤ —Å—Ä–µ–¥–µ
            next_state, reward, terminated, _ = env.step(action)
            total_reward += float(reward)

            # –†–∞—Å—á–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            step_duration = time.time() - step_start_time
            efficiency_calc.update(action, reward, len(detected_objects), step_duration)
            step_start_time = time.time()

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
            epsilon_agent.update_q_values(state, action, reward, next_state)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics.record_step(
                episode=episode,
                step=step,
                action=action,
                reward=reward,
                objects_detected=len(detected_objects),
                state=state
            )

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π –∏–≥—Ä–æ–∫–∞
            for obj in detected_objects:
                if obj['type'] == 'player':
                    episode_objects.append(obj['position'])

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –∫–∞–¥—Ä–∞
            if episode == 0 and step == 0 and len(detected_objects) > 0:
                frame = detector.visualize_detection(state, detected_objects)
                cv2.imwrite("screenshots/first_frame.png", frame)

            state = next_state

            if terminated:
                print(f"   üéÆ –ò–≥—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –Ω–∞ —à–∞–≥–µ {step}")
                break

        # –£–º–µ–Ω—å—à–µ–Ω–∏–µ epsilon
        epsilon_agent.epsilon = max(0.1, epsilon_agent.epsilon * 0.8)

        print(f"   –≠–ø–∏–∑–æ–¥ {episode + 1}: –Ω–∞–≥—Ä–∞–¥–∞={total_reward:5.1f}, "
              f"—à–∞–≥–æ–≤={step + 1:2d}, Œµ={epsilon_agent.epsilon:.3f}")

    training_time = time.time() - training_start
    print(f"   ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {training_time:.1f} —Å–µ–∫")

    # 6. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ê–ì–ï–ù–¢–û–í
    print("\nüß™ 6. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤...")

    test_results = {}
    agents_to_test = [("random", random_agent), ("epsilon_greedy", epsilon_agent)]

    for agent_name, agent in agents_to_test:
        print(f"\n   –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {agent_name} –∞–≥–µ–Ω—Ç–∞...")

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –≤–∏–¥–µ–æ—Ä–µ–∫–æ—Ä–¥–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞
        agent_video_recorder = VideoRecorder(
            filename=f"videos/{agent_name}_{timestamp}.mp4",
            fps=10.0,
            frame_size=(320, 320)
        )
        agent_video_recorder.start()

        # –î–ª—è –æ–±—É—á–µ–Ω–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ –æ—Ç–∫–ª—é—á–∞–µ–º exploration
        if agent_name == "epsilon_greedy":
            original_epsilon = agent.epsilon
            agent.epsilon = 0.0

        # –°–±—Ä–æ—Å –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∞
        test_threat_analyzer = ThreatAnalyzer()
        test_efficiency_calc = EfficiencyCalculator()

        state = env.reset()
        total_reward = 0.0
        positions = []
        frames_to_save = []

        for step in range(25):
            detected_objects = detector.detect(state)
            action = agent.get_action(state, step)
            next_state, reward, terminated, _ = env.step(action)
            total_reward += float(reward)

            # –ê–Ω–∞–ª–∏–∑ —É–≥—Ä–æ–∑
            test_threat_analyzer.analyze_frame(detected_objects, action, reward)

            # –†–∞—Å—á–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
            test_efficiency_calc.update(action, reward, len(detected_objects), 0.1)

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏ –∏–≥—Ä–æ–∫–∞
            for obj in detected_objects:
                if obj['type'] == 'player':
                    positions.append([int(obj['position'][0]), int(obj['position'][1])])

            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∑–∞–ø–∏—Å—å –∫–∞–¥—Ä–∞
            if len(detected_objects) > 0:
                frame = detector.visualize_detection(state, detected_objects)
                agent_video_recorder.record_frame(frame)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∫–∞–¥—Ä–æ–≤
                if step in [0, 8, 16, 24]:
                    frames_to_save.append((step, frame))

            state = next_state

            if terminated:
                break

        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å –≤–∏–¥–µ–æ
        agent_video_recorder.stop()

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º epsilon
        if agent_name == "epsilon_greedy":
            agent.epsilon = original_epsilon

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤
        for step_num, frame in frames_to_save:
            cv2.imwrite(f"screenshots/{agent_name}_step_{step_num:02d}.png", frame)

        # –†–∞—Å—á–µ—Ç –∏–Ω–¥–µ–∫—Å–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        efficiency_report = test_efficiency_calc.calculate_efficiency_index()
        threat_report = test_threat_analyzer.generate_threat_report()

        test_results[agent_name] = {
            'total_reward': float(total_reward),
            'steps': int(step + 1),
            'positions': positions,
            'efficiency_index': efficiency_report['efficiency_index'],
            'accuracy': efficiency_report['accuracy_percent'],
            'threat_miss_rate': threat_report['miss_rate_percent'],
            'efficiency_details': efficiency_report,
            'threat_details': threat_report
        }

        print(f"     –ù–∞–≥—Ä–∞–¥–∞: {total_reward:.1f}")
        print(f"     –®–∞–≥–æ–≤: {step + 1}")
        print(f"     –ò–Ω–¥–µ–∫—Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {efficiency_report['efficiency_index']:.1f}")
        print(f"     –¢–æ—á–Ω–æ—Å—Ç—å: {efficiency_report['accuracy_percent']:.1f}%")
        print(f"     –ü—Ä–æ–ø—É—â–µ–Ω–æ —É–≥—Ä–æ–∑: {threat_report['miss_rate_percent']:.1f}%")

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç—ã
        if positions:
            heatmap = visualizer.create_heatmap(
                positions,
                grid_size=(10, 10),
                title=f"–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞: {agent_name} –∞–≥–µ–Ω—Ç"
            )
            visualizer.save_visualization(
                heatmap,
                f"results/{agent_name}_heatmap.png"
            )

        # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞—Ä—Ç—ã —É–≥—Ä–æ–∑
        if threat_report['threat_map']:
            threat_positions = []
            for y in range(10):
                for x in range(10):
                    threat_level = int(threat_report['threat_map'][y][x])
                    for _ in range(threat_level):
                        threat_positions.append((x, y))

            if threat_positions:
                threat_heatmap = visualizer.create_heatmap(
                    threat_positions,
                    grid_size=(10, 10),
                    title=f"–ö–∞—Ä—Ç–∞ —É–≥—Ä–æ–∑: {agent_name} –∞–≥–µ–Ω—Ç"
                )
                visualizer.save_visualization(
                    threat_heatmap,
                    f"results/{agent_name}_threat_map.png"
                )

    # 7. –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò –ë–ï–ù–ß–ú–ê–†–ö–ò –ú–û–î–ï–õ–ï–ô (–ù–û–í–´–ô –≠–¢–ê–ü 2)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –≠—Ç–∞–ø–∞ 2 –ª–æ–∫–∞–ª—å–Ω–æ
    DQN_AVAILABLE_LOCAL = False
    TESTING_FRAMEWORK_AVAILABLE_LOCAL = False

    try:
        from agents.dqn_agent import DQNAgent
        DQN_AVAILABLE_LOCAL = True
    except ImportError:
        pass

    try:
        from utils.model_interface import ModelInterface
        from agents.agent_wrapper import AgentWrapper
        from object_detection.detector_wrapper import DetectorWrapper
        from analytics.inference_visualizer import InferenceVisualizer
        TESTING_FRAMEWORK_AVAILABLE_LOCAL = True
    except ImportError:
        pass

    if TESTING_FRAMEWORK_AVAILABLE_LOCAL:
        print("\nüß™ 7. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –±–µ–Ω—á–º–∞—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π...")

        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_state = env.reset()

        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –¥–µ—Ç–µ–∫—Ç–æ—Ä
        detector_wrapper = DetectorWrapper(detector, f"detector_{detector_type}")
        detector_benchmark = detector_wrapper.benchmark_inference(test_state, n_iterations=50)
        detector_consistency = detector_wrapper.test_consistency(test_state, n_runs=5)

        print(f"   üìä –î–µ—Ç–µ–∫—Ç–æ—Ä {detector_type}:")
        print(
            f"     –°–∫–æ—Ä–æ—Å—Ç—å: {detector_benchmark['avg_inference_time_ms']:.1f} –º—Å ({detector_benchmark['fps']:.1f} FPS)")
        print(f"     –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å: {'‚úÖ' if detector_consistency['all_results_match'] else '‚ùå'}")

        # –î–æ–±–∞–≤–ª—è–µ–º DQN –∞–≥–µ–Ω—Ç–∞ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        dqn_agent = None
        dqn_training_time = 0

        if DQN_AVAILABLE_LOCAL:
            print("\n   üß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DQN –∞–≥–µ–Ω—Ç–∞...")
            try:
                dqn_agent = DQNAgent(
                    input_shape=env.get_state_shape(),
                    num_actions=num_actions,
                    use_gpu=False
                )

                # –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ DQN (5 —ç–ø–∏–∑–æ–¥–æ–≤)
                print("   üìö –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ DQN (5 —ç–ø–∏–∑–æ–¥–æ–≤)...")
                dqn_training_start = time.time()

                for episode in range(5):
                    state = env.reset()
                    total_reward = 0

                    for step in range(20):
                        action = dqn_agent.get_action(state, step, training=True)
                        next_state, reward, terminated, _ = env.step(action)
                        total_reward += reward

                        dqn_agent.remember(state, action, reward, next_state, terminated)
                        dqn_agent.replay()

                        state = next_state
                        if terminated:
                            break

                    print(f"     –≠–ø–∏–∑–æ–¥ {episode + 1}: –Ω–∞–≥—Ä–∞–¥–∞={total_reward:.1f}, Œµ={dqn_agent.epsilon:.3f}")

                dqn_training_time = time.time() - dqn_training_start
                print(f"     ‚è±Ô∏è  –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è DQN: {dqn_training_time:.1f} —Å–µ–∫")

                # –î–æ–±–∞–≤–ª—è–µ–º DQN –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                agents_to_test.append(("dqn", dqn_agent))

            except Exception as e:
                print(f"   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DQN: {e}")
                DQN_AVAILABLE_LOCAL = False

        # –ë–µ–Ω—á–º–∞—Ä–∫–∏ –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤
        agent_benchmarks = {}
        for agent_name, agent in agents_to_test:
            wrapper = AgentWrapper(agent, agent_name)
            benchmark = wrapper.benchmark_inference(test_state, n_iterations=100)
            agent_benchmarks[agent_name] = benchmark

            print(f"   ü§ñ {agent_name}: {benchmark['avg_inference_time_ms']:.1f} –º—Å")

        # –ò–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        print("\n   üé® –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞...")
        visualizer_inf = InferenceVisualizer()

        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–≥–µ–Ω—Ç–∞ (–∫—Ä–æ–º–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ) —Å–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫—É —Ä–µ—à–µ–Ω–∏–π
        for agent_name, agent in agents_to_test:
            if agent_name != "random":
                try:
                    if hasattr(agent, 'get_inference_info'):
                        info = agent.get_inference_info(test_state)
                        fig = visualizer_inf.plot_q_values_decision(
                            info['q_values'],
                            info['chosen_action'],
                            title=f"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å {agent_name} –∞–≥–µ–Ω—Ç–∞"
                        )
                        fig.savefig(f"results/{agent_name}_inference.png", dpi=150, bbox_inches='tight')
                        plt.close(fig)
                        print(f"     ‚úÖ –ò–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {agent_name}_inference.png")
                    elif agent_name == "epsilon_greedy":
                        # –î–ª—è Œµ-greedy –∞–≥–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–µ–º —Å–≤–æ—é –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫—É
                        state_key = agent._state_to_key(test_state)
                        if state_key in agent.q_table:
                            q_values = agent.q_table[state_key]
                            fig = visualizer_inf.plot_q_values_decision(
                                q_values,
                                np.argmax(q_values),
                                title=f"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å {agent_name} –∞–≥–µ–Ω—Ç–∞ (Q-table)"
                            )
                            fig.savefig(f"results/{agent_name}_inference.png", dpi=150, bbox_inches='tight')
                            plt.close(fig)
                            print(f"     ‚úÖ –ò–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∞: {agent_name}_inference.png")
                except Exception as e:
                    print(f"     ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∏ –¥–ª—è {agent_name}: {e}")

        # –°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        print("\n   üìà –°–†–ê–í–ù–ò–¢–ï–õ–¨–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")
        print("   " + "=" * 60)
        print("   –ú–æ–¥–µ–ª—å                  –í—Ä–µ–º—è (–º—Å)    FPS     –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å")
        print("   " + "-" * 60)

        print(f"   –î–µ—Ç–µ–∫—Ç–æ—Ä ({detector_type:14}) {detector_benchmark['avg_inference_time_ms']:8.1f}     "
              f"{detector_benchmark['fps']:6.1f}     "
              f"{'‚úÖ' if detector_consistency['all_results_match'] else '‚ùå'}")

        for agent_name in sorted(agent_benchmarks.keys()):
            bench = agent_benchmarks[agent_name]
            consistency = "‚úÖ"  # –ê–≥–µ–Ω—Ç—ã –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω—ã –∏–ª–∏ Œµ=0
            print(f"   –ê–≥–µ–Ω—Ç {agent_name:17} {bench['avg_inference_time_ms']:8.1f}     "
                  f"{bench['fps']:6.1f}     {consistency}")

        print("   " + "=" * 60)

        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ DQN –∞–≥–µ–Ω—Ç–∞ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        if dqn_agent is not None:
            print(f"\n   üß† DQN –ê–ì–ï–ù–¢ –ü–†–û–¢–ï–°–¢–ò–†–û–í–ê–ù:")
            print(f"     –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: Conv2D(6‚Üí32‚Üí64) ‚Üí FC(‚Üí128‚Üí{num_actions})")
            print(f"     –†–∞–∑–º–µ—Ä replay buffer: {len(dqn_agent.memory)}")
            print(f"     –¢–µ–∫—É—â–∏–π epsilon: {dqn_agent.epsilon:.3f}")
            print(f"     –û–±—É—á–µ–Ω–∏–µ: {dqn_training_time:.1f} —Å–µ–∫, 5 —ç–ø–∏–∑–æ–¥–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
        benchmark_results = {
            'detector': detector_benchmark,
            'detector_consistency': detector_consistency,
            'agents': agent_benchmarks,
            'timestamp': timestamp
        }

        with open(f"results/benchmarks_{timestamp}.json", 'w', encoding='utf-8') as f:
            json.dump(benchmark_results, f, cls=NumpyEncoder, ensure_ascii=False, indent=2)

        print(f"   üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: results/benchmarks_{timestamp}.json")

    # 8. –ê–ù–ê–õ–ò–¢–ò–ö–ê –ò –°–û–•–†–ê–ù–ï–ù–ò–ï
    print("\nüìä 8. –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

    # –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    metrics.plot_training_progress("results/training_progress.png")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    epsilon_agent.save_model("models/trained_agent.pkl")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ DQN –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
    if 'dqn_agent' in locals() and dqn_agent is not None:
        try:
            dqn_agent.save_model("models/dqn_agent.pth")
            print(f"üíæ DQN –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/dqn_agent.pth")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è DQN –º–æ–¥–µ–ª–∏: {e}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–≥–µ–Ω—Ç–∞
    stats = epsilon_agent.get_stats()

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
    print("\n" + "=" * 80)
    print("‚úÖ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–†–û–ï–ö–¢–ê")
    print("=" * 80)

    # –°–æ–∑–¥–∞–Ω–∏–µ JSON –æ—Ç—á–µ—Ç–∞
    report_data = {
        "project_name": "–ê–Ω–∞–ª–∏–∑ –ø–æ–≤–µ–¥–µ–Ω–∏—è AI-–±–æ—Ç–æ–≤ –≤ 2D-—à—É—Ç–µ—Ä–µ",
        "timestamp": timestamp,
        "detector_type": detector_type,
        "game": {
            "name": "Space Invaders (MinAtar)",
            "num_actions": int(num_actions),
            "state_shape": [int(dim) for dim in env.get_state_shape()]
        },
        "training": {
            "episodes": 3,
            "training_time_seconds": float(training_time),
            "total_states_learned": int(stats['total_states']),
            "final_epsilon": float(epsilon_agent.epsilon)
        },
        "testing_results": test_results,
        "comparison": {
            "reward_improvement": float(
                test_results['epsilon_greedy']['total_reward'] - test_results['random']['total_reward']),
            "conclusion": "–û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ" if test_results['epsilon_greedy'][
                                                                             'total_reward'] > test_results['random'][
                                                                             'total_reward'] else "–¢—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏—è"
        },
        "stage_2_features": {
            "dqn_tested": dqn_agent is not None,
            "testing_framework_used": TESTING_FRAMEWORK_AVAILABLE_LOCAL,
            "agents_tested": len(agents_to_test)
        }
    }

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
    report_path = "results/final_report.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, cls=NumpyEncoder, ensure_ascii=False, indent=2)
    print(f"üíæ JSON –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
    text_report_path = "results/final_report.txt"
    with open(text_report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 70 + "\n")
        f.write("üìã –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–†–û–ï–ö–¢–ê\n")
        f.write("=" * 70 + "\n\n")

        f.write(f"–î–µ—Ç–µ–∫—Ç–æ—Ä: {detector_type}\n")
        f.write(f"–¢–µ—Å—Ç–æ–≤—ã–µ –æ–±—ä–µ–∫—Ç—ã: {len(test_objects)}\n\n")

        f.write("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø:\n")
        for agent_name, results in test_results.items():
            f.write(f"\n{agent_name.upper()} –∞–≥–µ–Ω—Ç:\n")
            f.write(f"  –ù–∞–≥—Ä–∞–¥–∞: {results['total_reward']:.1f}\n")
            f.write(f"  –®–∞–≥–æ–≤: {results['steps']}\n")
            f.write(f"  –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: {results['efficiency_index']:.1f}\n")
            f.write(f"  –¢–æ—á–Ω–æ—Å—Ç—å: {results['accuracy']:.1f}%\n")

        f.write(f"\n–°–†–ê–í–ù–ï–ù–ò–ï:\n")
        diff = test_results['epsilon_greedy']['total_reward'] - test_results['random']['total_reward']
        improvement = (diff / test_results['random']['total_reward'] * 100) if test_results['random'][
                                                                                   'total_reward'] > 0 else 0
        f.write(f"  –†–∞–∑–Ω–∏—Ü–∞ –≤ –Ω–∞–≥—Ä–∞–¥–µ: {diff:+.1f}\n")
        if test_results['random']['total_reward'] > 0:
            f.write(f"  –£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.1f}%\n")

        if diff > 0:
            f.write("  ‚úÖ –û–±—É—á–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç –ª—É—á—à–µ —Å–ª—É—á–∞–π–Ω–æ–≥–æ!\n")
        else:
            f.write("  ‚ö†Ô∏è –ù—É–∂–Ω–æ –±–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏—è\n")

        if TESTING_FRAMEWORK_AVAILABLE_LOCAL:
            f.write(f"\n–≠–¢–ê–ü 2 - –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô:\n")
            f.write(f"  –î–µ—Ç–µ–∫—Ç–æ—Ä: {detector_type}, {detector_benchmark['avg_inference_time_ms']:.1f} –º—Å\n")
            f.write(f"  –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–æ –∞–≥–µ–Ω—Ç–æ–≤: {len(agent_benchmarks)}\n")
            if dqn_agent is not None:
                f.write(f"  DQN –∞–≥–µ–Ω—Ç: –æ–±—É—á–µ–Ω –∑–∞ {dqn_training_time:.1f} —Å–µ–∫\n")

    print(f"\nüìÅ –°–û–ó–î–ê–ù–ù–´–ï –§–ê–ô–õ–´:")
    print(f"  results/final_report.json          - –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç")
    print(f"  results/final_report.txt           - –ö—Ä–∞—Ç–∫–∏–π –æ—Ç—á–µ—Ç")
    print(f"  results/training_progress.png      - –ì—Ä–∞—Ñ–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è")
    print(f"  results/*_heatmap.png              - –¢–µ–ø–ª–æ–≤—ã–µ –∫–∞—Ä—Ç—ã")
    print(f"  results/*_threat_map.png           - –ö–∞—Ä—Ç—ã —É–≥—Ä–æ–∑")
    if TESTING_FRAMEWORK_AVAILABLE_LOCAL:
        print(f"  results/*_inference.png          - –ò–Ω—Ñ–æ–≥—Ä–∞—Ñ–∏–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞")
        print(f"  results/benchmarks_*.json       - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
    print(f"  models/trained_agent.pkl           - –°–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å")
    if 'dqn_agent' in locals() and dqn_agent is not None:
        print(f"  models/dqn_agent.pth             - DQN –º–æ–¥–µ–ª—å")
    print(f"  videos/*.mp4                       - –í–∏–¥–µ–æ–∑–∞–ø–∏—Å–∏")
    print(f"  screenshots/*.png                  - –°–∫—Ä–∏–Ω—à–æ—Ç—ã")

    print(f"\nüèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"  üé≤ –°–ª—É—á–∞–π–Ω—ã–π –∞–≥–µ–Ω—Ç: {test_results['random']['total_reward']:.1f} –æ—á–∫–æ–≤")
    print(f"  üéØ Œµ-greedy –∞–≥–µ–Ω—Ç: {test_results['epsilon_greedy']['total_reward']:.1f} –æ—á–∫–æ–≤")

    if 'dqn_agent' in locals() and dqn_agent is not None:
        # –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç DQN –∞–≥–µ–Ω—Ç–∞
        dqn_test_state = env.reset()
        dqn_test_reward = 0
        for _ in range(10):
            action = dqn_agent.get_action(dqn_test_state, training=False)
            next_state, reward, terminated, _ = env.step(action)
            dqn_test_reward += reward
            dqn_test_state = next_state
            if terminated:
                break
        print(f"  üß† DQN –∞–≥–µ–Ω—Ç: {dqn_test_reward:.1f} –æ—á–∫–æ–≤ (–±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç)")

    print(
        f"  üìà –£–ª—É—á—à–µ–Ω–∏–µ Œµ-greedy: +{test_results['epsilon_greedy']['total_reward'] - test_results['random']['total_reward']:+.1f}")

    print(f"\n‚úÖ –ü–†–û–ï–ö–¢ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")


if __name__ == "__main__":
    run_final_project()
    